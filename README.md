# ZigCUDA - Native CUDA API for Zig

A comprehensive native CUDA API binding for the Zig programming language, providing direct access to NVIDIA's CUDA runtime and libraries without the overhead of C FFI.

## ðŸŽ¯ Key Features

- **Native Zig Integration**: Direct bindings to CUDA Driver API, cuBLAS, and cuRNN
- **Type-Safe Kernel Launch**: Compile-time verification of kernel parameters
- **Memory Pool Management**: Efficient GPU memory allocation and management
- **Quantized Model Support**: Native support for INT4/INT8 quantized models (GPTQ, AWQ)
- **Production-Ready**: HTTP server with OpenAI-compatible API
- **Single Binary Deployment**: No external dependencies beyond NVIDIA driver

## ðŸ“Š Performance Characteristics

| Metric | vLLM | TRT-LLM | ZigCUDA |
|--------|------|----------|----------|
| Binary Size | ~500MB+ | ~1GB+ | **<5MB** |
| Startup Time | 10-30s | 30-60s | **<1s** |
| Memory Safety | Manual | Manual | **Compile-time** |

## ðŸ—ï¸ Architecture

### Core Components

- **CUDA Bindings** (`src/bindings/`): CUDA Driver API, cuBLAS, type definitions, error mapping
- **Core Runtime** (`src/core/`): Device management, context handling, memory pools, kernel launching
- **Tensor Operations** (`src/ops/`): Matrix multiplication, attention mechanisms, normalization, activations
- **Library Integrations** (`src/integrations/`): cuBLAS wrapper, Marlin INT4 kernels, Flash Attention
- **Model Loading**: Safetensors, GPTQ, AWQ format support
- **Inference Engine**: KV cache management, continuous batching, production server

## ðŸš€ Quick Start

### Prerequisites
- Zig compiler (latest stable)
- NVIDIA driver (CUDA toolkit not required for runtime)
- NVIDIA GPU with compute capability 6.0+

### Building

```bash
# Debug build
zig build

# Release build for production
zig build -Doptimize=ReleaseFast
```

### Basic Usage

```zig
const cuda = @import("cuda");
const tensor = @import("tensor");

pub fn main() !void {
    // Initialize CUDA
    try cuda.init();
    
    // Get first GPU device
    var device = try cuda.Device.init(0);
    
    // Create context
    var ctx = try cuda.Context.init(&device);
    
    // Load and run inference
    // ... see examples/ directory
}
```

### Command Line Usage

```bash
# Start server
./zigcuda serve --model ./llama-7b-gptq --port 8080

# Benchmark performance
./zigcuda bench --model ./llama-7b-gptq --batch-size 32

# Model information
./zigcuda info --model ./llama-7b-gptq
```

## ðŸ¤– Model Loading

Supports multiple model formats:

- **Safetensors**: Standard PyTorch model format
- **GPTQ**: Quantized models with INT4 dequantization
- **AWQ**: Alternative quantization format

## ðŸŒ HTTP API

OpenAI-compatible API endpoints:
- `POST /v1/chat/completions` - Chat completions
- `POST /v1/completions` - Text completions  
- `GET /v1/models` - List available models
- `GET /health` - Health check

## ðŸŽ¯ Target Use Cases

- **Edge AI deployments**: Where binary size and startup time are critical
- **Embedded systems**: Single binary, no Python runtime required
- **High-frequency trading**: Minimal latency, deterministic performance
- **Kernel researchers**: Direct access to launch custom CUDA kernels
- **Production teams**: Simpler deployment than Python stacks

## ðŸ“ Project Structure

```
src/
â”œâ”€â”€ bindings/          # CUDA API bindings
â”œâ”€â”€ core/             # Core runtime components
â”œâ”€â”€ ops/              # Tensor operations
â”œâ”€â”€ integrations/     # Library integrations
â””â”€â”€ main.zig         # Entry point
```

## ðŸ› ï¸ Development Status

- [x] Phase 0: Driver Bindings
- [x] Phase 1: Core Runtime
- [x] Phase 2: Kernel Integration
- [x] Phase 3: Tensor Layer
- [x] Phase 4: Model Loading
- [ ] Phase 5: Inference Engine (In Progress)
- [ ] Phase 6: Production Serving

## ðŸ¤ Contributing

Contributions are welcome! Please see the contributing guidelines and ensure all tests pass before submitting PRs.

## ðŸ“œ License

[License information to be added]

---

*Bringing high-performance AI inference to resource-constrained environments with Zig's safety and performance benefits.*