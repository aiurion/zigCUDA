# ZigCUDA - Native CUDA API for Zig

A comprehensive native CUDA API binding for the Zig programming language, providing direct access to NVIDIA's CUDA runtime and libraries without the overhead of C FFI.

> **Status**: Research Prototype - Solid foundation with 86+ tests passing, ready for experimentation and contribution

## ğŸ¯ Key Features (Implemented)

- **Native Zig Integration**: Direct bindings to CUDA Driver API with dynamic loading
- **Type-Safe Kernel Launch**: Compile-time verification of kernel parameters  
- **Memory Management**: Efficient GPU memory allocation and resource management
- **Comprehensive Testing**: 86+ unit tests covering core functionality
- **Zero External Dependencies**: No Python runtime, no C FFI complexity

## ğŸ“Š Current Status

| Component | Tests Passing | Implementation |
|-----------|---------------|------------------|
| CUDA Driver API | âœ… 44/46 | Production quality bindings |
| Kernel System | âœ… 23/23 | Full type-safe kernel launching |  
| Runtime Core | âœ… 13/13 | Memory/streams/events working |
| cuBLAS Integration | ï¿½ï¸ Partial | Library loads, symbol resolution in progress |

## ğŸ—ï¸ Architecture

### Core Components (Implemented)

- **CUDA Bindings** (`src/bindings/`): CUDA Driver API with comprehensive error handling and dynamic loading
- **Core Runtime** (`src/core/`): Device management, context handling, memory pools, type-safe kernel launching  
- **Testing Infrastructure**: Comprehensive unit test suite covering all major components
- **Library Integrations** (`src/integrations/`): cuBLAS wrapper (partial implementation)

### Planned Components

- **Tensor Operations** (`src/ops/`): Matrix multiplication, attention mechanisms, normalization, activations
- **Model Loading**: Safetensors, GPTQ, AWQ format support
- **Inference Engine**: KV cache management, continuous batching, production server
- **Library Integrations**: Marlin INT4 kernels, Flash Attention

## ğŸš€ Quick Start

### Prerequisites
- Zig compiler (latest stable)
- NVIDIA driver (CUDA toolkit not required for runtime)
- NVIDIA GPU with compute capability 6.0+

### Building

```bash
# Debug build
zig build

# Release build for production
zig build -Doptimize=ReleaseFast
```

### Basic Usage

```zig
const cuda = @import("cuda");
const tensor = @import("tensor");

pub fn main() !void {
    // Initialize CUDA
    try cuda.init();
    
    // Get first GPU device
    var device = try cuda.Device.init(0);
    
    // Create context
    var ctx = try cuda.Context.init(&device);
    
    // Load and run inference
    // ... see examples/ directory
}
```

### Command Line Usage

```bash
# Start server
./zigcuda serve --model ./llama-7b-gptq --port 8080

# Benchmark performance
./zigcuda bench --model ./llama-7b-gptq --batch-size 32

# Model information
./zigcuda info --model ./llama-7b-gptq
```

## ğŸ¤– Planned Features (Not Yet Implemented)

The following features are planned for future releases:

- **Model Loading**: Support for Safetensors, GPTQ, AWQ formats
- **Tensor Operations**: Matrix multiplication, attention mechanisms, normalization  
- **Inference Engine**: KV cache management and batching
- **HTTP API**: OpenAI-compatible endpoints for serving models
- **Command Line Tools**: Model serving and benchmarking utilities

## ğŸ¯ Current Use Cases

This research prototype is currently useful for:

- **CUDA Researchers**: Direct native bindings without C FFI complexity
- **Zig Developers**: Type-safe kernel launching with compile-time validation  
- **GPU Programmers**: Testing and experimenting with CUDA operations
- **Embedded Systems Research**: Exploring lightweight CUDA runtime alternatives
- **Performance Experiments**: Benchmarking native vs Python-based stacks

*Note: Production deployment not recommended until model loading and inference engine are implemented.*

## ğŸ“ Project Structure

```
src/
â”œâ”€â”€ bindings/          # CUDA API bindings
â”œâ”€â”€ core/             # Core runtime components
â”œâ”€â”€ ops/              # Tensor operations
â”œâ”€â”€ integrations/     # Library integrations
â””â”€â”€ main.zig         # Entry point
```

## ğŸ› ï¸ Development Status

### âœ… Completed Phases
- **Phase 0: Driver Bindings** - Complete with 44/46 comprehensive tests passing
- **Phase 1: Core Runtime** - Complete with 13/13 memory/streams/events working  
- **Phase 2: Kernel Integration** - Complete with 23/23 type-safe kernel launching

### ğŸ”§ In Progress
- **cuBLAS Integration**: Library loads successfully, symbol resolution in progress

### ï¿½ï¸ Planned Phases (Not Started)
- **Tensor Layer**: Matrix operations, attention mechanisms, normalization layers
- **Model Loading**: Support for Safetensors, GPTQ, AWQ formats  
- **Inference Engine**: KV cache management and batching system
- **Production Serving**: HTTP API server with OpenAI-compatible endpoints

## ğŸ¤ Contributing

Contributions are welcome! Please see the contributing guidelines and ensure all tests pass before submitting PRs.

## ğŸ“œ License

[License information to be added]

---

*A research prototype for native CUDA bindings in Zig, providing type-safe access to GPU computation without C FFI complexity. Perfect for experimentation and development of GPU-accelerated applications.*

*Current focus: Solid foundation with comprehensive testing suite - production AI inference features coming in future releases.*